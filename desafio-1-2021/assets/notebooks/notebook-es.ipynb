{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Desafío 1 - Maratón Behind the Code 2021\n",
    "\n",
    "### Notebook guia\n",
    "\n",
    "Este Jupyter Notebook te dará instrucciones para crear una solución introductoria al Desafío 1 de la Maratón. ¡Siéntete libre de editar y mejorar tu solución!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ten en cuenta: si estás utilizando Watson Studio, recuerda hacer que tu Notebook sea editable haciendo clic en el botón de edición de arriba.**\n",
    "\n",
    "![](https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/edit-notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de dataset\n",
    "El primer paso para desarrollar un buen modelo de aprendizaje automático es explorar los datos con los que tenemos que trabajar. Debemos comprender lo mejor posible la relevancia de cada dato para el valor que queremos predecir. Después de todo, la predicción del modelo se basa completamente en los datos con los que se entrenó.\n",
    "\n",
    "Hay muchas bibliotecas de Python que se pueden utilizar para el procesamiento y la visualización de datos. En este caso usaremos Pandas, Seaborn y Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, cargamos el conjunto de datos del desafío en este notebook. Comencemos con el principal, `LOANS.csv`. Para eso, podemos usar el ícono de activos, disponible en la esquina superior derecha de la pantalla, e insertar el conjunto de datos como un DataFrame Pandas, como en la imagen de abajo.\n",
    "\n",
    "<img width=\"300px\" src=\"https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/load-loans.png\" />\n",
    "\n",
    "Repite este procedimiento para todos los datasets que vas a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cargue aqui el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambia el nombre de la variable creada con el conjunto de datos para `loans`, para cumplir con los códigos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = df_cargado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar los métodos .info () y .describe () para obtener información básica sobre la cantidad actual de datos, sus tipos y valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable de destino para este desafío es `ALLOW`, es decir, si se debe permitir o no un préstamo, según la información proporcionada. Echemos un vistazo a cómo se distribuye esta variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_plot = sns.countplot(data=loans, x='ALLOW', order=loans['ALLOW'].value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siéntete libre de ver la distribución de otras columnas en el conjunto de datos, usar los otros conjuntos de datos, explorar correlaciones entre variables y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione suas explorações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos explorado los datos, comprendemos la importancia de cada columna y podemos hacer cambios en ellas para obtener un mejor resultado. Aquí, vamos a hacer un procesamiento simple, para eliminar del conjunto de datos las líneas a las que les falta algún valor. Esta técnica no es necesariamente la mejor para usar en el desafío, es solo un ejemplo de cómo manejar el conjunto de datos.\n",
    "\n",
    "Para procesamientos más avanzados, como modificar columnas o crear columnas nuevas, ve a continuación en Notebook, donde explicamos cómo usar `Pipelines`, desde la biblioteca `sklearn`, para realizar transformaciones de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_df = loans.dropna()\n",
    "clean_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que ahora temos un dataset \"limpio\", pero perdimos algunos datos al eliminar filas donde faltaba al menos una columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar la ejecución del método `.info()` anterior, podemos ver que hay tres columnas de tipo `object`. El modelo `scikit-learn` que vamos a utilizar no es capaz de procesar dicha variable. Entonces, para continuar con el experimento, eliminemos esta columna. Recomendamos que utilice alguna técnica para manejar variables categóricas, como _one-hot encoding_, en lugar de eliminar la columna.\n",
    "\n",
    "También eliminaremos la columna `ID`, ya que sabemos que no es información útil para la predicción (es solo un número que identifica a un cliente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = ['INSTALLMENT_PLANS', 'LOAN_PURPOSE', 'OTHERS_ON_LOAN']\n",
    "clean_df = clean_df.drop(object_columns, axis=1)\n",
    "clean_df = clean_df.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo\n",
    "\n",
    "Con los datos listos, podemos seleccionar un modelo de Machine Learning para entrenar con nuestros datos. En este ejemplo, vamos a utilizar un modelo de clasificación básico, el árbol de decisiones.\n",
    "\n",
    "Para poder evaluar el rendimiento de nuestro modelo, dividamos los datos que tenemos entre los datos de entrenamiento y de prueba, y luego, después del entrenamiento, veremos cómo le va con las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, separamos los datos que queremos predecir de los datos que usamos como información para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['PAYMENT_TERM', 'INSTALLMENT_PERCENT', 'LOAN_AMOUNT']\n",
    "target = ['ALLOW']\n",
    "\n",
    "X = clean_df[features]\n",
    "y = clean_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pct = 0.3 # Separaremos 30% de los dados para testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_pct)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Exactitud del modelo (número de predicciones asertadas sobre el número total de pruebas): {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque estamos usando solo unas pocas variables del conjunto de datos cargado, el desafío espera un modelo que acepte todas las variables de los conjuntos de datos disponibles. Entonces, usemos un transformador para transformar los datos de entrada, eliminando las columnas que no queremos, antes de enviarlo a nuestro modelo. Por lo tanto, vamos a crear un `Pipeline`, que usa el transformador como entrada, y nuestro modelo a continuación.\n",
    "\n",
    "Depende de ti unir los otros conjuntos de datos disponibles y usarlos también para las predicciones en el modelo, en lugar de eliminar las columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre Pipelines\n",
    "\n",
    "Un `Pipeline`, de la librería `scikit-learn`, consta de una serie de pasos donde realizamos transformaciones de datos. Las transformaciones están definidas por clases que siempre deben tener **dos métodos**:\n",
    "\n",
    "- **fit**: Un método que recibe datos de entrenamiento y devuelve la propia instancia de clase. Se aplica cuando se entrena para usar una canalización para entrenar un modelo.\n",
    "- **transform**: Un método que toma un conjunto de datos como entrada y debe devolver otro conjunto de datos, transformado. Se aplica a cada paso del Pipeline, recibiendo los datos del paso anterior y transformándolos.\n",
    "\n",
    "Vea a continuación una representación gráfica de cómo funciona un Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/pipeline-es.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este Notebook, creamos un Pipeline muy similar al ejemplo anterior, con dos etapas:\n",
    "\n",
    "- **drop_columns**: Remueve las columnas indeseadas del conjunto de datos de entrada.\n",
    "- **classification**: Alimenta un modelo de clasificación con los datos obtenidos de la etapa de **drop_columns**, pudiendo ser tanto para entrenamiento como para obtener una predicción. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de Pipelines con scikit-learn\n",
    "\n",
    "Para crear un modelo capaz de hacer transformaciones con los datos de entrada, vamos a crear un `Pipeline` de `scikit-learn` y aplicar nuestras transformaciones dentro de sus etapas.\n",
    "\n",
    "A continuación, definimos un transformador de ejemplo, que eliminará las columnas pasadas como parámetro en su inicialización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Un transformador para remover columnas indeseadas\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Primero realizamos la cópia del DataFrame 'X' de entrada\n",
    "        data = X.copy()\n",
    "        # Retornamos um nuevo dataframe sin las colunmas indeseadas\n",
    "        return data.drop(labels=self.columns, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto el método `fit` como `transform` deben definirse, incluso si no van a hacer nada diferente, como en el caso de ese `fit`.\n",
    "\n",
    "Asimismo, puedes crear otros transformadores, para otros fines, siempre heredando de las clases `BaseEstimator` y `TransformerMixin`. Puedes utilizar un transformador para, por ejemplo, crear nuevas columnas, editar tipos de datos de columnas existentes, etc.\n",
    "\n",
    "Ahora, vamos a crear un Pipeline para utilizar nuestro modelo, conservando todas las columnas esperadas para el desafío y removiendo las que no queremos usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_columns = ['ID', 'CHECKING_BALANCE', 'PAYMENT_TERM', 'CREDIT_HISTORY',\n",
    "       'LOAN_PURPOSE', 'LOAN_AMOUNT', 'EXISTING_SAVINGS',\n",
    "       'EMPLOYMENT_DURATION', 'INSTALLMENT_PERCENT', 'SEX', 'OTHERS_ON_LOAN',\n",
    "       'CURRENT_RESIDENCE_DURATION', 'PROPERTY', 'AGE', 'INSTALLMENT_PLANS',\n",
    "       'HOUSING', 'EXISTING_CREDITS_COUNT', 'JOB_TYPE', 'DEPENDENTS',\n",
    "       'TELEPHONE', 'FOREIGN_WORKER']\n",
    "\n",
    "unwanted_columns = list((set(challenge_columns) - set(target)) - set(features)) # Remover todas las colunmas que no son features do nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una instancia del transformador, pasando como parámetro las colunmas que no queremos\n",
    "drop_columns = DropColumns(unwanted_columns)\n",
    "\n",
    "\n",
    "# Creando un Pipeline, adicionando nuestro transformador seguido de un modelo de árbol de decisión\n",
    "skl_pipeline = Pipeline(steps=[('drop_columns', drop_columns), ('classification', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Listo! Este Pipeline ahora está listo para recibir todas las variables de desafío, aunque el modelo solo use algunas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy del modelo para Watson Machine Learning (WML)\n",
    "\n",
    "Ahora que tenemos el modelo listo para su publicación, queremos ponerlo en línea para que el sistema de la Maratón pueda probarlo :)\n",
    "\n",
    "Para ello, utilizaremos la biblioteca `IBM Watson Machine Learning`, que le permite encapsular modelos de Machine Learning en APIs REST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar la biblioteca WML\n",
    "!pip install -U ibm-watson-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si aún no lo ha creado, cree un servicio de Machine Learning aquí https://cloud.ibm.com/catalog/services/machine-learning.\n",
    "\n",
    "Ingresa tus credenciales para el servicio en la celda a continuación.\n",
    "\n",
    "En `location`, ingresa el ID de la región donde se encuentra su servicio de WML instanciado, de acuerdo con las posibilidades de abajo:\n",
    "\n",
    "-\tDallas - `us-south`\n",
    "-\tLondon - `eu-gb`\n",
    "-\tFrankfurt - `eu-de`\n",
    "-\tTokyo - `jp-tok`\n",
    "\n",
    "Para la API key, generela aqui: https://cloud.ibm.com/iam/apikeys. No la compartas con nadie! Una API key da acesso a su cuenta de IBM Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'INSIRA SUA API KEY AQUI'\n",
    "location = 'us-south' # En caso de WML estar en una región diferente, altere esta linea\n",
    "\n",
    "wml_credentials = {\n",
    "    \"apikey\": api_key,\n",
    "    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n",
    "}\n",
    "\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un espacio para guardar tu modelo. Puedes crearlo aqui: https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\n",
    "\n",
    "Cuando crees tu espacio, **asocia la instancia de tu servicio de WML al espacio!** Sin asociar, no conseguiras efectuar el deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista espacios creados en su instancia de WML\n",
    "client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia el ID de tu espacio creado para el desafío y pégualo a continuación para usarlo. Deberías ver el mensaje 'SUCCESS' si el espacio está configurado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_id = 'cole aqui'\n",
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilización de Pipeline dentro de Watson Machine Learning (WML)\n",
    "\n",
    "Para utilizar un Pipeline en WML con transformadores customizados, son necesarios algunos pasos adicionales:\n",
    "\n",
    "1.\tCrea un paquete en Python que contenga el transformador personalizado.\n",
    "2.\tCarga ese paquete con el transformador en un repositório en WML;\n",
    "3.\tCrea una especificación de software, con este paquete personalizado, que se utilizará como tiempo de ejecución del modelo en WML.\n",
    "\n",
    "Como exemplo, vamos utilizar un paquete ya listo, disponible aqui: https://github.com/vnderlev/watson-sklearn-transforms. Para configurar un paquete de Python, son necesarios algunos otros archivos, pero la lógica del transformador creado se encuentra en [este archivo](https://github.com/vnderlev/watson-sklearn-transforms/blob/master/my_custom_sklearn_transforms/sklearn_transformers.py). En este caso, este es el mismo transformador que definimos aquí, excluirá del conjunto de datos las columnas pasadas como parámetros en su inicialización.\n",
    "\n",
    "Abajo, vamos a bajar este paquete de GitHub e instalarlo en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf watson-sklearn-transforms # Remover carpeta en caso de que ya exista\n",
    "!git clone https://github.com/vnderlev/watson-sklearn-transforms # Clonar el repositório con el pacote\n",
    "!zip -r drop-columns.zip watson-sklearn-transforms # Zippear el paquete\n",
    "!pip install drop-columns.zip # Instalar el paquete zippeado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a recrear nuestro Pipeline utilizando este paquete instalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_custom_sklearn_transforms.sklearn_transformers import DropColumns\n",
    "\n",
    "drop_columns = DropColumns(unwanted_columns)\n",
    "\n",
    "pipeline = Pipeline(steps=[('drop_columns', drop_columns), ('classification', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a subir el transformador customizado que bajamos para WML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatos para el paquete customizado\n",
    "meta_prop_pkg_extn = {\n",
    "    client.package_extensions.ConfigurationMetaNames.NAME: \"Drop_Columns\",\n",
    "    client.package_extensions.ConfigurationMetaNames.DESCRIPTION: \"Extensión para remover columnas\",\n",
    "    client.package_extensions.ConfigurationMetaNames.TYPE: \"pip_zip\"\n",
    "}\n",
    "\n",
    "# Subir el paquete\n",
    "pkg_extn_details = client.package_extensions.store(meta_props=meta_prop_pkg_extn, file_path=\"drop-columns.zip\")\n",
    "\n",
    "# Guardar las informaciones sobre el paquete\n",
    "pkg_extn_uid = client.package_extensions.get_uid(pkg_extn_details)\n",
    "pkg_extn_url = client.package_extensions.get_href(pkg_extn_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creemos una especificación de software con nuestro paquete personalizado para que lo use WML. Si estás utilizando un software que no sea `Python 3.8` o una biblioteca que no sea `scikit-learn`, puedes consultar la lista de especificaciones de software compatibles con WML: https://dataplatform.cloud.ibm.com/docs/content/wsj/wmls/wmls-deploy-python-types.html?context=analytics&audience=wdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.8\")\n",
    "\n",
    "# Si desea utilizar un software que no sea Python 3.8 como base, eche un vistazo a los disponibles con la línea a continuación\n",
    "# client.software_specifications.list(limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatos de la nueva especificación de software\n",
    "meta_prop_sw_spec = {\n",
    "    client.software_specifications.ConfigurationMetaNames.NAME: \"sw_spec_drop_columns\",\n",
    "    client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Software specification for DropColumns\",\n",
    "    client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\"guid\": base_sw_spec_uid}\n",
    "}\n",
    "\n",
    "# Creando la nueva especificacion de software y obteniendo su ID\n",
    "sw_spec_details = client.software_specifications.store(meta_props=meta_prop_sw_spec)\n",
    "sw_spec_uid = client.software_specifications.get_uid(sw_spec_details)\n",
    "\n",
    "# Agregar el paquete personalizado a la nueva especificación\n",
    "client.software_specifications.add_package_extension(sw_spec_uid, pkg_extn_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a publicar el pipeline utilizando la especificación de software customizada que creamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatos del modelo\n",
    "model_props = {\n",
    "    client.repository.ModelMetaNames.NAME: \"Modelo com Pipeline customizada\",\n",
    "    client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_uid\n",
    "}\n",
    "\n",
    "# Publicando el Pipeline como um modelo\n",
    "published_model = client.repository.store_model(model=pipeline, meta_props=model_props)\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "client.repository.get_details(published_model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su modelo ahora está guardado. Vamos ahora a dejarlo disponible online, para que podamos testearlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatos para publicación del modelo\n",
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Publicación de modelo customizado\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "# Publicar\n",
    "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Felicitaciones!\n",
    "\n",
    "Su modelo está ahora publicado. Cuando este listo para enviar el desafio, puedes acceder a https://maratona.dev/challenge/1, y utilizar las credenciales abajo para realizar la entrega. Recuerda revisar todas las instrucciones en el [README](https://github.com/maratonadev/desafio-1-2021) antes de entregar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "\n",
    "print(f\"Credenciales para el envio (no compartir estos datos con nadie!)\\n\\nAPI key: {api_key}\\nDeployment ID: {deployment_uid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
